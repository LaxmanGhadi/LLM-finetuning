{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":6063,"sourceType":"modelInstanceVersion","modelInstanceId":4684,"modelId":2820}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:20:49.612505Z","iopub.execute_input":"2025-12-17T15:20:49.613001Z","iopub.status.idle":"2025-12-17T15:20:49.863483Z","shell.execute_reply.started":"2025-12-17T15:20:49.612976Z","shell.execute_reply":"2025-12-17T15:20:49.862858Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llm-classification-finetuning/sample_submission.csv\n/kaggle/input/llm-classification-finetuning/train.csv\n/kaggle/input/llm-classification-finetuning/test.csv\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/config.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/tokenizer.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/metadata.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/model.weights.h5\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/assets/tokenizer/vocabulary.spm\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf \nfrom tensorflow import keras \nimport keras_nlp\nimport tqdm\nimport json\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport plotly.express as px","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:20:49.866247Z","iopub.execute_input":"2025-12-17T15:20:49.866529Z","iopub.status.idle":"2025-12-17T15:21:08.476399Z","shell.execute_reply.started":"2025-12-17T15:20:49.866511Z","shell.execute_reply":"2025-12-17T15:21:08.475806Z"}},"outputs":[{"name":"stderr","text":"2025-12-17 15:20:51.289895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765984851.486960      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765984851.543297      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":3},{"cell_type":"markdown","source":"\nCheck out models for finetuning supported by keras\nhttps://keras.io/keras_hub/api/models/ ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed = 40 \n    model = \"deberta_v3_extra_small_en\"\n    sequence_len = 512\n    epochs  = 3 # needs only 1-3 epochs to avoid overfitting\n    batch_size = 16 #\n    scheduler  = 'cosine'\n    labelname = { 0 : 'winner_model_a' ,1 : 'winner_model_b',2 : 'winner_tie'  }\n    namelabel = {v:k for k,v in labelname.items()}\n    classlabel= list(labelname.keys())\n    classnames = list(labelname.values())\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:21:08.477592Z","iopub.execute_input":"2025-12-17T15:21:08.478169Z","iopub.status.idle":"2025-12-17T15:21:08.482912Z","shell.execute_reply.started":"2025-12-17T15:21:08.478142Z","shell.execute_reply":"2025-12-17T15:21:08.482313Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:21:08.483588Z","iopub.execute_input":"2025-12-17T15:21:08.483863Z","iopub.status.idle":"2025-12-17T15:21:08.501074Z","shell.execute_reply.started":"2025-12-17T15:21:08.483823Z","shell.execute_reply":"2025-12-17T15:21:08.500475Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"keras.mixed_precision.set_global_policy(\"mixed_float16\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:21:08.502550Z","iopub.execute_input":"2025-12-17T15:21:08.502745Z","iopub.status.idle":"2025-12-17T15:21:08.512530Z","shell.execute_reply.started":"2025-12-17T15:21:08.502729Z","shell.execute_reply":"2025-12-17T15:21:08.511747Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/llm-classification-finetuning'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:21:08.513444Z","iopub.execute_input":"2025-12-17T15:21:08.513607Z","iopub.status.idle":"2025-12-17T15:21:08.523153Z","shell.execute_reply.started":"2025-12-17T15:21:08.513587Z","shell.execute_reply":"2025-12-17T15:21:08.522475Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df = pd.read_csv(f'{BASE_PATH}/train.csv') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:21:08.523859Z","iopub.execute_input":"2025-12-17T15:21:08.524204Z","iopub.status.idle":"2025-12-17T15:21:11.407988Z","shell.execute_reply.started":"2025-12-17T15:21:08.524178Z","shell.execute_reply":"2025-12-17T15:21:11.407364Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df.head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:21:11.408715Z","iopub.execute_input":"2025-12-17T15:21:11.408922Z","iopub.status.idle":"2025-12-17T15:21:11.435349Z","shell.execute_reply.started":"2025-12-17T15:21:11.408906Z","shell.execute_reply":"2025-12-17T15:21:11.434618Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"      id             model_a     model_b  \\\n0  30192  gpt-4-1106-preview  gpt-4-0613   \n1  53567           koala-13b  gpt-4-0613   \n\n                                              prompt  \\\n0  [\"Is it morally right to try to have a certain...   \n1  [\"What is the difference between marriage lice...   \n\n                                          response_a  \\\n0  [\"The question of whether it is morally right ...   \n1  [\"A marriage license is a legal document that ...   \n\n                                          response_b  winner_model_a  \\\n0  [\"As an AI, I don't have personal beliefs or o...               1   \n1  [\"A marriage license and a marriage certificat...               0   \n\n   winner_model_b  winner_tie  \n0               0           0  \n1               1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"df.prompt[4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:21:11.436119Z","iopub.execute_input":"2025-12-17T15:21:11.436434Z","iopub.status.idle":"2025-12-17T15:21:11.587881Z","shell.execute_reply.started":"2025-12-17T15:21:11.436410Z","shell.execute_reply":"2025-12-17T15:21:11.587282Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'[\"What is the best way to travel from Tel-Aviv to Jerusalem? Car? Bus? Plane?\"]'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Load Train Data\ndf = pd.read_csv(f'{BASE_PATH}/train.csv') \n\n\n# Prompts are shown as string representation of a list or tuple in which the prompt is at it's 0th position\ndf[\"prompt\"] = df.prompt.map(lambda x: eval(x)[0])\ndf[\"response_a\"] = df.response_a.map(lambda x: eval(x.replace(\"null\",\"''\"))[0])\ndf[\"response_b\"] = df.response_b.map(lambda x: eval(x.replace(\"null\", \"''\"))[0])\n\n# Label conversion\ndf[\"class_name\"] = df[[\"winner_model_a\", \"winner_model_b\" , \"winner_tie\"]].idxmax(axis=1)\ndf[\"class_label\"] = df.class_name.map(CFG.namelabel)\ndf['label_arr'] = df[[\"winner_model_a\", \"winner_model_b\" , \"winner_tie\"]].values.tolist()\n# Show Sample\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:21:11.588685Z","iopub.execute_input":"2025-12-17T15:21:11.588947Z","iopub.status.idle":"2025-12-17T15:21:16.902816Z","shell.execute_reply.started":"2025-12-17T15:21:11.588918Z","shell.execute_reply":"2025-12-17T15:21:16.902067Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"       id             model_a              model_b  \\\n0   30192  gpt-4-1106-preview           gpt-4-0613   \n1   53567           koala-13b           gpt-4-0613   \n2   65089  gpt-3.5-turbo-0613       mistral-medium   \n3   96401    llama-2-13b-chat  mistral-7b-instruct   \n4  198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                              prompt  \\\n0  Is it morally right to try to have a certain p...   \n1  What is the difference between marriage licens...   \n2  explain function calling. how would you call a...   \n3  How can I create a test set for a very rare ca...   \n4  What is the best way to travel from Tel-Aviv t...   \n\n                                          response_a  \\\n0  The question of whether it is morally right to...   \n1  A marriage license is a legal document that al...   \n2  Function calling is the process of invoking or...   \n3  Creating a test set for a very rare category c...   \n4  The best way to travel from Tel Aviv to Jerusa...   \n\n                                          response_b  winner_model_a  \\\n0  As an AI, I don't have personal beliefs or opi...               1   \n1  A marriage license and a marriage certificate ...               0   \n2  Function calling is the process of invoking a ...               0   \n3  When building a classifier for a very rare cat...               1   \n4  The best way to travel from Tel-Aviv to Jerusa...               0   \n\n   winner_model_b  winner_tie      class_name  class_label  label_arr  \n0               0           0  winner_model_a            0  [1, 0, 0]  \n1               1           0  winner_model_b            1  [0, 1, 0]  \n2               0           1      winner_tie            2  [0, 0, 1]  \n3               0           0  winner_model_a            0  [1, 0, 0]  \n4               1           0  winner_model_b            1  [0, 1, 0]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n      <th>class_name</th>\n      <th>class_label</th>\n      <th>label_arr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>Is it morally right to try to have a certain p...</td>\n      <td>The question of whether it is morally right to...</td>\n      <td>As an AI, I don't have personal beliefs or opi...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>winner_model_a</td>\n      <td>0</td>\n      <td>[1, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>What is the difference between marriage licens...</td>\n      <td>A marriage license is a legal document that al...</td>\n      <td>A marriage license and a marriage certificate ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>winner_model_b</td>\n      <td>1</td>\n      <td>[0, 1, 0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>explain function calling. how would you call a...</td>\n      <td>Function calling is the process of invoking or...</td>\n      <td>Function calling is the process of invoking a ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>winner_tie</td>\n      <td>2</td>\n      <td>[0, 0, 1]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>How can I create a test set for a very rare ca...</td>\n      <td>Creating a test set for a very rare category c...</td>\n      <td>When building a classifier for a very rare cat...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>winner_model_a</td>\n      <td>0</td>\n      <td>[1, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>What is the best way to travel from Tel-Aviv t...</td>\n      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>winner_model_b</td>\n      <td>1</td>\n      <td>[0, 1, 0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Load Test Data\ntest_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n\n# Take the first prompt and response\ntest_df[\"prompt\"] = test_df.prompt.map(lambda x: eval(x)[0])\ntest_df[\"response_a\"] = test_df.response_a.map(lambda x: eval(x.replace(\"null\",\"''\"))[0])\ntest_df[\"response_b\"] = test_df.response_b.map(lambda x: eval(x.replace(\"null\", \"''\"))[0])\n\n# Show Sample\ntest_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:21:16.905237Z","iopub.execute_input":"2025-12-17T15:21:16.905552Z","iopub.status.idle":"2025-12-17T15:21:16.921671Z","shell.execute_reply.started":"2025-12-17T15:21:16.905530Z","shell.execute_reply":"2025-12-17T15:21:16.921009Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"        id                                             prompt  \\\n0   136060  I have three oranges today, I ate an orange ye...   \n1   211333  You are a mediator in a heated political debat...   \n2  1233961  How to initialize the classification head when...   \n\n                                          response_a  \\\n0                        You have two oranges today.   \n1  Thank you for sharing the details of the situa...   \n2  When you want to initialize the classification...   \n\n                                          response_b  \n0  You still have three oranges. Eating an orange...  \n1  Mr Reddy and Ms Blue both have valid points in...  \n2  To initialize the classification head when per...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>I have three oranges today, I ate an orange ye...</td>\n      <td>You have two oranges today.</td>\n      <td>You still have three oranges. Eating an orange...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>You are a mediator in a heated political debat...</td>\n      <td>Thank you for sharing the details of the situa...</td>\n      <td>Mr Reddy and Ms Blue both have valid points in...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>How to initialize the classification head when...</td>\n      <td>When you want to initialize the classification...</td>\n      <td>To initialize the classification head when per...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"\n# Define a function to create options based on the prompt and choices\ndef make_pairs_ts(row):\n    row[\"encode_fail\"] = False\n    try:\n        prompt = row.prompt.encode(\"utf-8\").decode(\"utf-8\")\n    except:\n        prompt = \"\"\n        row[\"encode_fail\"] = True\n\n    try:\n        response_a = row.response_a.encode(\"utf-8\").decode(\"utf-8\")\n    except:\n        response_a = \"\"\n        row[\"encode_fail\"] = True\n\n    try:\n        response_b = row.response_b.encode(\"utf-8\").decode(\"utf-8\")\n    except:\n        response_b = \"\"\n        row[\"encode_fail\"] = True\n        \n    row['options'] = [f\"Prompt: {prompt}\",f\"\\n\\nResponse_a: {response_a}\",  # Response from Model A\n                      f\"Response_b: {response_b}\"  # Response from Model B\n                     ]\n    return row\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:21:16.922277Z","iopub.execute_input":"2025-12-17T15:21:16.922460Z","iopub.status.idle":"2025-12-17T15:21:16.988581Z","shell.execute_reply.started":"2025-12-17T15:21:16.922446Z","shell.execute_reply":"2025-12-17T15:21:16.988038Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def make_pairs_tr(row):\n    row[\"encode_fail\"] = False\n    win_arr= row['class_name']\n    try:\n        prompt = row.prompt.encode(\"utf-8\").decode(\"utf-8\")\n    except:\n        prompt = \"\"\n        row[\"encode_fail\"] = True\n\n    try:\n        response_a = row.response_a.encode(\"utf-8\").decode(\"utf-8\")\n    except:\n        response_a = \"\"\n        row[\"encode_fail\"] = True\n\n    try:\n        response_b = row.response_b.encode(\"utf-8\").decode(\"utf-8\")\n    except:\n        response_b = \"\"\n        row[\"encode_fail\"] = True\n        \n    row['options'] = [f\"Prompt: {prompt}\",f\"\\n\\nResponse_a: {response_a}\",  # Response from Model A\n                      f\"Response_b: {response_b}\",f\"winner:{win_arr}\"  # Response from Model B\n                     ]\n    return row","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:21:16.989228Z","iopub.execute_input":"2025-12-17T15:21:16.989465Z","iopub.status.idle":"2025-12-17T15:21:16.998706Z","shell.execute_reply.started":"2025-12-17T15:21:16.989444Z","shell.execute_reply":"2025-12-17T15:21:16.998107Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df = df.apply(make_pairs_tr, axis=1)  # Apply the make_pairs function to each row in df\ndisplay(df.head(2))  # Display the first 2 rows of df\n\ntest_df = test_df.apply(make_pairs_ts, axis=1)  # Apply the make_pairs function to each row in df\ndisplay(test_df.head(2))  # Display the first 2 rows of df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:21:16.999297Z","iopub.execute_input":"2025-12-17T15:21:16.999454Z","iopub.status.idle":"2025-12-17T15:22:02.105510Z","shell.execute_reply.started":"2025-12-17T15:21:16.999442Z","shell.execute_reply":"2025-12-17T15:22:02.104721Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"      id             model_a     model_b  \\\n0  30192  gpt-4-1106-preview  gpt-4-0613   \n1  53567           koala-13b  gpt-4-0613   \n\n                                              prompt  \\\n0  Is it morally right to try to have a certain p...   \n1  What is the difference between marriage licens...   \n\n                                          response_a  \\\n0  The question of whether it is morally right to...   \n1  A marriage license is a legal document that al...   \n\n                                          response_b  winner_model_a  \\\n0  As an AI, I don't have personal beliefs or opi...               1   \n1  A marriage license and a marriage certificate ...               0   \n\n   winner_model_b  winner_tie      class_name  class_label  label_arr  \\\n0               0           0  winner_model_a            0  [1, 0, 0]   \n1               1           0  winner_model_b            1  [0, 1, 0]   \n\n   encode_fail                                            options  \n0        False  [Prompt: Is it morally right to try to have a ...  \n1        False  [Prompt: What is the difference between marria...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n      <th>class_name</th>\n      <th>class_label</th>\n      <th>label_arr</th>\n      <th>encode_fail</th>\n      <th>options</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>Is it morally right to try to have a certain p...</td>\n      <td>The question of whether it is morally right to...</td>\n      <td>As an AI, I don't have personal beliefs or opi...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>winner_model_a</td>\n      <td>0</td>\n      <td>[1, 0, 0]</td>\n      <td>False</td>\n      <td>[Prompt: Is it morally right to try to have a ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>What is the difference between marriage licens...</td>\n      <td>A marriage license is a legal document that al...</td>\n      <td>A marriage license and a marriage certificate ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>winner_model_b</td>\n      <td>1</td>\n      <td>[0, 1, 0]</td>\n      <td>False</td>\n      <td>[Prompt: What is the difference between marria...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       id                                             prompt  \\\n0  136060  I have three oranges today, I ate an orange ye...   \n1  211333  You are a mediator in a heated political debat...   \n\n                                          response_a  \\\n0                        You have two oranges today.   \n1  Thank you for sharing the details of the situa...   \n\n                                          response_b  encode_fail  \\\n0  You still have three oranges. Eating an orange...        False   \n1  Mr Reddy and Ms Blue both have valid points in...        False   \n\n                                             options  \n0  [Prompt: I have three oranges today, I ate an ...  \n1  [Prompt: You are a mediator in a heated politi...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>encode_fail</th>\n      <th>options</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>I have three oranges today, I ate an orange ye...</td>\n      <td>You have two oranges today.</td>\n      <td>You still have three oranges. Eating an orange...</td>\n      <td>False</td>\n      <td>[Prompt: I have three oranges today, I ate an ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>You are a mediator in a heated political debat...</td>\n      <td>Thank you for sharing the details of the situa...</td>\n      <td>Mr Reddy and Ms Blue both have valid points in...</td>\n      <td>False</td>\n      <td>[Prompt: You are a mediator in a heated politi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"df.encode_fail.value_counts(normalize=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:22:02.107113Z","iopub.execute_input":"2025-12-17T15:22:02.107344Z","iopub.status.idle":"2025-12-17T15:22:02.117211Z","shell.execute_reply.started":"2025-12-17T15:22:02.107327Z","shell.execute_reply":"2025-12-17T15:22:02.116668Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"encode_fail\nFalse    56885\nTrue       592\nName: count, dtype: int64"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:22:02.118068Z","iopub.execute_input":"2025-12-17T15:22:02.118464Z","iopub.status.idle":"2025-12-17T15:22:02.137248Z","shell.execute_reply.started":"2025-12-17T15:22:02.118435Z","shell.execute_reply":"2025-12-17T15:22:02.136583Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"       id             model_a              model_b  \\\n0   30192  gpt-4-1106-preview           gpt-4-0613   \n1   53567           koala-13b           gpt-4-0613   \n2   65089  gpt-3.5-turbo-0613       mistral-medium   \n3   96401    llama-2-13b-chat  mistral-7b-instruct   \n4  198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                              prompt  \\\n0  Is it morally right to try to have a certain p...   \n1  What is the difference between marriage licens...   \n2  explain function calling. how would you call a...   \n3  How can I create a test set for a very rare ca...   \n4  What is the best way to travel from Tel-Aviv t...   \n\n                                          response_a  \\\n0  The question of whether it is morally right to...   \n1  A marriage license is a legal document that al...   \n2  Function calling is the process of invoking or...   \n3  Creating a test set for a very rare category c...   \n4  The best way to travel from Tel Aviv to Jerusa...   \n\n                                          response_b  winner_model_a  \\\n0  As an AI, I don't have personal beliefs or opi...               1   \n1  A marriage license and a marriage certificate ...               0   \n2  Function calling is the process of invoking a ...               0   \n3  When building a classifier for a very rare cat...               1   \n4  The best way to travel from Tel-Aviv to Jerusa...               0   \n\n   winner_model_b  winner_tie      class_name  class_label  label_arr  \\\n0               0           0  winner_model_a            0  [1, 0, 0]   \n1               1           0  winner_model_b            1  [0, 1, 0]   \n2               0           1      winner_tie            2  [0, 0, 1]   \n3               0           0  winner_model_a            0  [1, 0, 0]   \n4               1           0  winner_model_b            1  [0, 1, 0]   \n\n   encode_fail                                            options  \n0        False  [Prompt: Is it morally right to try to have a ...  \n1        False  [Prompt: What is the difference between marria...  \n2        False  [Prompt: explain function calling. how would y...  \n3        False  [Prompt: How can I create a test set for a ver...  \n4        False  [Prompt: What is the best way to travel from T...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n      <th>class_name</th>\n      <th>class_label</th>\n      <th>label_arr</th>\n      <th>encode_fail</th>\n      <th>options</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>Is it morally right to try to have a certain p...</td>\n      <td>The question of whether it is morally right to...</td>\n      <td>As an AI, I don't have personal beliefs or opi...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>winner_model_a</td>\n      <td>0</td>\n      <td>[1, 0, 0]</td>\n      <td>False</td>\n      <td>[Prompt: Is it morally right to try to have a ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>What is the difference between marriage licens...</td>\n      <td>A marriage license is a legal document that al...</td>\n      <td>A marriage license and a marriage certificate ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>winner_model_b</td>\n      <td>1</td>\n      <td>[0, 1, 0]</td>\n      <td>False</td>\n      <td>[Prompt: What is the difference between marria...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>explain function calling. how would you call a...</td>\n      <td>Function calling is the process of invoking or...</td>\n      <td>Function calling is the process of invoking a ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>winner_tie</td>\n      <td>2</td>\n      <td>[0, 0, 1]</td>\n      <td>False</td>\n      <td>[Prompt: explain function calling. how would y...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>How can I create a test set for a very rare ca...</td>\n      <td>Creating a test set for a very rare category c...</td>\n      <td>When building a classifier for a very rare cat...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>winner_model_a</td>\n      <td>0</td>\n      <td>[1, 0, 0]</td>\n      <td>False</td>\n      <td>[Prompt: How can I create a test set for a ver...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>What is the best way to travel from Tel-Aviv t...</td>\n      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>winner_model_b</td>\n      <td>1</td>\n      <td>[0, 1, 0]</td>\n      <td>False</td>\n      <td>[Prompt: What is the best way to travel from T...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"df.iloc[0]['options']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:22:02.137990Z","iopub.execute_input":"2025-12-17T15:22:02.138589Z","iopub.status.idle":"2025-12-17T15:22:02.147105Z","shell.execute_reply.started":"2025-12-17T15:22:02.138551Z","shell.execute_reply":"2025-12-17T15:22:02.146425Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['Prompt: Is it morally right to try to have a certain percentage of females on managerial positions?',\n '\\n\\nResponse_a: The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\\n\\nHere are some arguments in favor of and against such policies:\\n\\n**Arguments in favor:**\\n\\n1. **Correcting Historical Inequities:** Women have historically been underrepresented in leadership roles due to various cultural, institutional, and social barriers. Aiming for a specific percentage can be seen as a corrective measure to address past and ongoing discrimination.\\n\\n2. **Promoting Diversity:** Diverse leadership teams can enhance decision-making and represent a broader range of perspectives. This can lead to better outcomes for organizations and society as a whole.\\n\\n3. **Equality of Opportunity:** Setting targets for female representation in management can help ensure that women have equal opportunities to advance in their careers.\\n\\n4. **Role Modeling:** Increased visibility of female leaders can inspire and encourage other women and girls to pursue leadership roles.\\n\\n**Arguments against:**\\n\\n1. **Reverse Discrimination:** Setting a quota for female representation might lead to the perception or reality of reverse discrimination, where men might be overlooked for positions despite being qualified, simply to meet the gender target.\\n\\n2. **Meritocracy Concerns:** Critics argue that employment and promotion should be based solely on merit and qualifications, not gender. They fear that quotas could compromise the quality of leadership if less qualified individuals are promoted to meet gender targets.\\n\\n3. **Tokenism:** There is a risk that women appointed to meet quotas might be seen as \"tokens,\" undermining their legitimacy and potentially leading to resentment among colleagues.\\n\\n4. **Oversimplification of Diversity:** Focusing only on gender might overlook other important aspects of diversity, such as race, ethnicity, socioeconomic background, or sexual orientation.\\n\\nUltimately, the morality of aiming for a certain percentage of females in managerial positions depends on one\\'s ethical framework and the context in which such policies are implemented. Proponents of gender diversity targets often argue that these measures are necessary as a transitional mechanism to create a level playing field, while opponents may argue for a \"color-blind\" or \"gender-blind\" approach to hiring and promotions.\\n\\nOrganizations and societies must carefully consider the implications of these strategies and strive to implement policies that promote fairness and equality while minimizing negative consequences. It is also important to address the root causes of gender inequality, such as unconscious bias, work-life balance challenges, and lack of supportive policies, to create an environment where everyone has the opportunity to succeed based on their capabilities and contributions.',\n \"Response_b: As an AI, I don't have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decision-making. Critics may argue that positions should be based purely on merit, and that quotas could potentially lead to tokenism or unqualified candidates being promoted. Morality can be subjective and differs from person to person based on their personal beliefs, cultural background, and values. It's a topic with valid arguments on both sides, and it's up to each individual or organization to decide what they believe is right.\",\n 'winner:winner_model_a']"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"model_df = pd.concat([df.model_a, df.model_b])\ncounts = model_df.value_counts().reset_index()\ncounts.columns = ['LLM','Count']\n\nfig = px.bar(counts ,x='LLM', y ='Count',\n             title = 'Distribution of LLMs',\n             color = 'Count' , color_continuous_scale = 'viridis'\n            )\nfig.update_layout(xaxis_tickangle = -45)\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:22:02.147751Z","iopub.execute_input":"2025-12-17T15:22:02.148064Z","iopub.status.idle":"2025-12-17T15:22:04.844265Z","shell.execute_reply.started":"2025-12-17T15:22:02.148036Z","shell.execute_reply":"2025-12-17T15:22:04.843592Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"7c34c190-d00d-4e11-b480-5cc423683df9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7c34c190-d00d-4e11-b480-5cc423683df9\")) {                    Plotly.newPlot(                        \"7c34c190-d00d-4e11-b480-5cc423683df9\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"LLM=%{x}\\u003cbr\\u003eCount=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[7387,7083,6165,5583,4136,4122,3978,3720,3545,3448,3428,3352,3315,2607,2456,2401,1977,1793,1644,1632,1617,1598,1591,1580,1494,1486,1474,1447,1438,1420,1403,1302,1261,1250,1200,1160,1158,1134,1072,1021,989,952,928,914,899,861,800,795,771,684,667,598,564,551,547,412,408,373,325,286,244,208,200,100],\"coloraxis\":\"coloraxis\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"gpt-4-1106-preview\",\"gpt-3.5-turbo-0613\",\"gpt-4-0613\",\"claude-2.1\",\"claude-instant-1\",\"gpt-4-0314\",\"claude-1\",\"vicuna-33b\",\"mixtral-8x7b-instruct-v0.1\",\"vicuna-13b\",\"llama-2-70b-chat\",\"gpt-3.5-turbo-1106\",\"mistral-medium\",\"llama-2-13b-chat\",\"claude-2.0\",\"zephyr-7b-beta\",\"palm-2\",\"llama-2-7b-chat\",\"wizardlm-70b\",\"openchat-3.5\",\"mistral-7b-instruct\",\"koala-13b\",\"vicuna-7b\",\"wizardlm-13b\",\"oasst-pythia-12b\",\"gemini-pro-dev-api\",\"codellama-34b-instruct\",\"yi-34b-chat\",\"gemini-pro\",\"pplx-70b-online\",\"alpaca-13b\",\"gpt-3.5-turbo-0314\",\"chatglm-6b\",\"pplx-7b-online\",\"tulu-2-dpo-70b\",\"gpt-4-0125-preview\",\"RWKV-4-Raven-14B\",\"starling-lm-7b-alpha\",\"qwen-14b-chat\",\"fastchat-t5-3b\",\"chatglm3-6b\",\"openhermes-2.5-mistral-7b\",\"mpt-7b-chat\",\"stripedhyena-nous-7b\",\"solar-10.7b-instruct-v1.0\",\"gpt-3.5-turbo-0125\",\"dolly-v2-12b\",\"deepseek-llm-67b-chat\",\"stablelm-tuned-alpha-7b\",\"guanaco-33b\",\"llama2-70b-steerlm-chat\",\"mpt-30b-chat\",\"chatglm2-6b\",\"qwen1.5-72b-chat\",\"llama-13b\",\"zephyr-7b-alpha\",\"gpt4all-13b-snoozy\",\"dolphin-2.2.1-mistral-7b\",\"nous-hermes-2-mixtral-8x7b-dpo\",\"falcon-180b-chat\",\"openchat-3.5-0106\",\"qwen1.5-7b-chat\",\"qwen1.5-4b-chat\",\"mistral-7b-instruct-v0.2\"],\"xaxis\":\"x\",\"y\":[7387,7083,6165,5583,4136,4122,3978,3720,3545,3448,3428,3352,3315,2607,2456,2401,1977,1793,1644,1632,1617,1598,1591,1580,1494,1486,1474,1447,1438,1420,1403,1302,1261,1250,1200,1160,1158,1134,1072,1021,989,952,928,914,899,861,800,795,771,684,667,598,564,551,547,412,408,373,325,286,244,208,200,100],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"LLM\"},\"tickangle\":-45},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Count\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Count\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Distribution of LLMs\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('7c34c190-d00d-4e11-b480-5cc423683df9');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                            </script>        </div>\n</body>\n</html>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:22:04.845301Z","iopub.execute_input":"2025-12-17T15:22:04.845607Z","iopub.status.idle":"2025-12-17T15:22:04.854866Z","shell.execute_reply.started":"2025-12-17T15:22:04.845587Z","shell.execute_reply":"2025-12-17T15:22:04.854220Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                         LLM  Count\n0         gpt-4-1106-preview   7387\n1         gpt-3.5-turbo-0613   7083\n2                 gpt-4-0613   6165\n3                 claude-2.1   5583\n4           claude-instant-1   4136\n..                       ...    ...\n59          falcon-180b-chat    286\n60         openchat-3.5-0106    244\n61           qwen1.5-7b-chat    208\n62           qwen1.5-4b-chat    200\n63  mistral-7b-instruct-v0.2    100\n\n[64 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LLM</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gpt-4-1106-preview</td>\n      <td>7387</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>7083</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>gpt-4-0613</td>\n      <td>6165</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>claude-2.1</td>\n      <td>5583</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>claude-instant-1</td>\n      <td>4136</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>falcon-180b-chat</td>\n      <td>286</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>openchat-3.5-0106</td>\n      <td>244</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>qwen1.5-7b-chat</td>\n      <td>208</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>qwen1.5-4b-chat</td>\n      <td>200</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>mistral-7b-instruct-v0.2</td>\n      <td>100</td>\n    </tr>\n  </tbody>\n</table>\n<p>64 rows  2 columns</p>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"win_resps['resps'] = pd.concat([df[df['class_name']=='winner_model_a']['response_a'],df[df['class_name']=='winner_model_b']['response_b']])\nwin_resps['lens'] = win_resps['resps'].apply(lambda x:len(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:46:21.040475Z","iopub.execute_input":"2025-12-17T15:46:21.041137Z","iopub.status.idle":"2025-12-17T15:46:21.096087Z","shell.execute_reply.started":"2025-12-17T15:46:21.041111Z","shell.execute_reply":"2025-12-17T15:46:21.095505Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"len_freq = win_resps['lens'].value_counts().sort_index()\nplt.hist(len_freq , color='skyblue', edgecolor='black')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:46:24.919289Z","iopub.execute_input":"2025-12-17T15:46:24.919903Z","iopub.status.idle":"2025-12-17T15:46:25.086134Z","shell.execute_reply.started":"2025-12-17T15:46:24.919879Z","shell.execute_reply":"2025-12-17T15:46:25.085300Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"(array([3.65e+03, 3.48e+02, 1.00e+00, 4.00e+00, 0.00e+00, 0.00e+00,\n        0.00e+00, 0.00e+00, 0.00e+00, 1.00e+00]),\n array([  1. ,  20.6,  40.2,  59.8,  79.4,  99. , 118.6, 138.2, 157.8,\n        177.4, 197. ]),\n <BarContainer object of 10 artists>)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:45:23.745118Z","iopub.execute_input":"2025-11-30T13:45:23.745402Z","iopub.status.idle":"2025-11-30T13:45:23.749506Z","shell.execute_reply.started":"2025-11-30T13:45:23.745383Z","shell.execute_reply":"2025-11-30T13:45:23.748668Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size = 0.2 , stratify = df['class_label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:45:24.507363Z","iopub.execute_input":"2025-11-30T13:45:24.508078Z","iopub.status.idle":"2025-11-30T13:45:24.575353Z","shell.execute_reply.started":"2025-11-30T13:45:24.508053Z","shell.execute_reply":"2025-11-30T13:45:24.574537Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:42:10.282586Z","iopub.execute_input":"2025-11-30T13:42:10.282877Z","iopub.status.idle":"2025-11-30T13:42:10.294288Z","shell.execute_reply.started":"2025-11-30T13:42:10.282856Z","shell.execute_reply":"2025-11-30T13:42:10.293624Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"      id             model_a         model_b  \\\n0  30192  gpt-4-1106-preview      gpt-4-0613   \n1  53567           koala-13b      gpt-4-0613   \n2  65089  gpt-3.5-turbo-0613  mistral-medium   \n\n                                              prompt  \\\n0  Is it morally right to try to have a certain p...   \n1  What is the difference between marriage licens...   \n2  explain function calling. how would you call a...   \n\n                                          response_a  \\\n0  The question of whether it is morally right to...   \n1  A marriage license is a legal document that al...   \n2  Function calling is the process of invoking or...   \n\n                                          response_b  winner_model_a  \\\n0  As an AI, I don't have personal beliefs or opi...               1   \n1  A marriage license and a marriage certificate ...               0   \n2  Function calling is the process of invoking a ...               0   \n\n   winner_model_b  winner_tie      class_name  class_label  encode_fail  \\\n0               0           0  winner_model_a            0        False   \n1               1           0  winner_model_b            1        False   \n2               0           1      winner_tie            2        False   \n\n                                             options  \n0  [Prompt: Is it morally right to try to have a ...  \n1  [Prompt: What is the difference between marria...  \n2  [Prompt: explain function calling. how would y...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n      <th>class_name</th>\n      <th>class_label</th>\n      <th>encode_fail</th>\n      <th>options</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>Is it morally right to try to have a certain p...</td>\n      <td>The question of whether it is morally right to...</td>\n      <td>As an AI, I don't have personal beliefs or opi...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>winner_model_a</td>\n      <td>0</td>\n      <td>False</td>\n      <td>[Prompt: Is it morally right to try to have a ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>What is the difference between marriage licens...</td>\n      <td>A marriage license is a legal document that al...</td>\n      <td>A marriage license and a marriage certificate ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>winner_model_b</td>\n      <td>1</td>\n      <td>False</td>\n      <td>[Prompt: What is the difference between marria...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>explain function calling. how would you call a...</td>\n      <td>Function calling is the process of invoking or...</td>\n      <td>Function calling is the process of invoking a ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>winner_tie</td>\n      <td>2</td>\n      <td>False</td>\n      <td>[Prompt: explain function calling. how would y...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"preprocessor  = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n    preset = CFG.model,\n    sequence_length=CFG.sequence_len,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:40:38.205037Z","iopub.execute_input":"2025-11-30T13:40:38.205311Z","iopub.status.idle":"2025-11-30T13:40:39.397757Z","shell.execute_reply.started":"2025-11-30T13:40:38.205292Z","shell.execute_reply":"2025-11-30T13:40:39.397151Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"outs = preprocessor(df.options.iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:40:39.398791Z","iopub.execute_input":"2025-11-30T13:40:39.399103Z","iopub.status.idle":"2025-11-30T13:40:39.505177Z","shell.execute_reply.started":"2025-11-30T13:40:39.399077Z","shell.execute_reply":"2025-11-30T13:40:39.504568Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"for k, v in outs.items():\n    print(k, \":\", v.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:41:19.971769Z","iopub.execute_input":"2025-11-30T13:41:19.972507Z","iopub.status.idle":"2025-11-30T13:41:19.976263Z","shell.execute_reply.started":"2025-11-30T13:41:19.972484Z","shell.execute_reply":"2025-11-30T13:41:19.975625Z"}},"outputs":[{"name":"stdout","text":"token_ids : (2, 512)\npadding_mask : (2, 512)\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# [\n#   {\n#     \"prompt\": \"What is the capital of France?\",\n#     \"response_a\": \"Paris is the capital of France.\",\n#     \"response_b\": \"The capital of France is Paris.\",\n#     \"winner_model_a\": 1,\n#     \"winner_model_b\": 0,\n#     \"winner_tie\": 0\n#   },]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_fn(text, label=None):\n    text = preprocessor(text)  # Preprocess text\n    return (text, label) if label is not None else text  # Return processed text and label if available","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:41:39.838560Z","iopub.execute_input":"2025-11-30T13:41:39.839238Z","iopub.status.idle":"2025-11-30T13:41:39.842972Z","shell.execute_reply.started":"2025-11-30T13:41:39.839206Z","shell.execute_reply":"2025-11-30T13:41:39.842366Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"train_texts = train_df.options.tolist()  # Extract training texts\ntrain_labels = train_df.class_label.tolist()  # Extract training labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:41:49.857489Z","iopub.execute_input":"2025-11-30T13:41:49.858086Z","iopub.status.idle":"2025-11-30T13:41:49.864265Z","shell.execute_reply.started":"2025-11-30T13:41:49.858062Z","shell.execute_reply":"2025-11-30T13:41:49.863518Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"valid_texts = val_df.options.tolist()  # Extract validation texts\nvalid_labels = val_df.class_label.tolist()  # Extract validation labels\nvalid_ds = build_dataset(valid_texts, valid_labels,\n                         batch_size=CFG.batch_size,\n                         shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:45:37.402609Z","iopub.execute_input":"2025-11-30T13:45:37.403512Z","iopub.status.idle":"2025-11-30T13:45:37.422957Z","shell.execute_reply.started":"2025-11-30T13:45:37.403486Z","shell.execute_reply":"2025-11-30T13:45:37.422058Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/146096370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvalid_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Extract validation texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalid_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Extract validation labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m valid_ds = build_dataset(valid_texts, valid_labels,\n\u001b[0m\u001b[1;32m      4\u001b[0m                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          shuffle=False)\n","\u001b[0;31mNameError\u001b[0m: name 'build_dataset' is not defined"],"ename":"NameError","evalue":"name 'build_dataset' is not defined","output_type":"error"}],"execution_count":47},{"cell_type":"code","source":"df.iloc[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:16:20.254964Z","iopub.execute_input":"2025-11-30T14:16:20.255526Z","iopub.status.idle":"2025-11-30T14:16:20.261592Z","shell.execute_reply.started":"2025-11-30T14:16:20.255505Z","shell.execute_reply":"2025-11-30T14:16:20.261027Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"id                                                            30192\nmodel_a                                          gpt-4-1106-preview\nmodel_b                                                  gpt-4-0613\nprompt            Is it morally right to try to have a certain p...\nresponse_a        The question of whether it is morally right to...\nresponse_b        As an AI, I don't have personal beliefs or opi...\nwinner_model_a                                                    1\nwinner_model_b                                                    0\nwinner_tie                                                        0\nclass_name                                           winner_model_a\nclass_label                                                       0\nencode_fail                                                   False\noptions           [Prompt: Is it morally right to try to have a ...\nName: 0, dtype: object"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"df.options[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:16:41.240039Z","iopub.execute_input":"2025-11-30T14:16:41.240321Z","iopub.status.idle":"2025-11-30T14:16:41.245490Z","shell.execute_reply.started":"2025-11-30T14:16:41.240299Z","shell.execute_reply":"2025-11-30T14:16:41.244779Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"['Prompt: Is it morally right to try to have a certain percentage of females on managerial positions?\\n\\nResponse: The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\\n\\nHere are some arguments in favor of and against such policies:\\n\\n**Arguments in favor:**\\n\\n1. **Correcting Historical Inequities:** Women have historically been underrepresented in leadership roles due to various cultural, institutional, and social barriers. Aiming for a specific percentage can be seen as a corrective measure to address past and ongoing discrimination.\\n\\n2. **Promoting Diversity:** Diverse leadership teams can enhance decision-making and represent a broader range of perspectives. This can lead to better outcomes for organizations and society as a whole.\\n\\n3. **Equality of Opportunity:** Setting targets for female representation in management can help ensure that women have equal opportunities to advance in their careers.\\n\\n4. **Role Modeling:** Increased visibility of female leaders can inspire and encourage other women and girls to pursue leadership roles.\\n\\n**Arguments against:**\\n\\n1. **Reverse Discrimination:** Setting a quota for female representation might lead to the perception or reality of reverse discrimination, where men might be overlooked for positions despite being qualified, simply to meet the gender target.\\n\\n2. **Meritocracy Concerns:** Critics argue that employment and promotion should be based solely on merit and qualifications, not gender. They fear that quotas could compromise the quality of leadership if less qualified individuals are promoted to meet gender targets.\\n\\n3. **Tokenism:** There is a risk that women appointed to meet quotas might be seen as \"tokens,\" undermining their legitimacy and potentially leading to resentment among colleagues.\\n\\n4. **Oversimplification of Diversity:** Focusing only on gender might overlook other important aspects of diversity, such as race, ethnicity, socioeconomic background, or sexual orientation.\\n\\nUltimately, the morality of aiming for a certain percentage of females in managerial positions depends on one\\'s ethical framework and the context in which such policies are implemented. Proponents of gender diversity targets often argue that these measures are necessary as a transitional mechanism to create a level playing field, while opponents may argue for a \"color-blind\" or \"gender-blind\" approach to hiring and promotions.\\n\\nOrganizations and societies must carefully consider the implications of these strategies and strive to implement policies that promote fairness and equality while minimizing negative consequences. It is also important to address the root causes of gender inequality, such as unconscious bias, work-life balance challenges, and lack of supportive policies, to create an environment where everyone has the opportunity to succeed based on their capabilities and contributions.',\n \"Prompt: Is it morally right to try to have a certain percentage of females on managerial positions?\\n\\nResponse: As an AI, I don't have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decision-making. Critics may argue that positions should be based purely on merit, and that quotas could potentially lead to tokenism or unqualified candidates being promoted. Morality can be subjective and differs from person to person based on their personal beliefs, cultural background, and values. It's a topic with valid arguments on both sides, and it's up to each individual or organization to decide what they believe is right.\"]"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}