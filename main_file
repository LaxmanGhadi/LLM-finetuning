{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":6063,"sourceType":"modelInstanceVersion","modelInstanceId":4684,"modelId":2820}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:08:15.515200Z","iopub.execute_input":"2025-12-21T13:08:15.515478Z","iopub.status.idle":"2025-12-21T13:08:17.061115Z","shell.execute_reply.started":"2025-12-21T13:08:15.515451Z","shell.execute_reply":"2025-12-21T13:08:17.060331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf \nfrom tensorflow import keras \nimport keras_nlp\nimport tqdm\nimport json\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport plotly.express as px","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:08:43.002914Z","iopub.execute_input":"2025-12-21T13:08:43.003581Z","iopub.status.idle":"2025-12-21T13:08:43.007108Z","shell.execute_reply.started":"2025-12-21T13:08:43.003560Z","shell.execute_reply":"2025-12-21T13:08:43.006469Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nCheck out models for finetuning supported by keras\nhttps://keras.io/keras_hub/api/models/ ","metadata":{}},{"cell_type":"markdown","source":"Step by step <br>\n\nData structure > prompt(response_a, response_b)->Winner(0,1,2) <br>\n**Step 1 :**<br>\ncreate a new column for winner class which should show the winner class in text . we have received one hot encoding \nThen we club together prompt - response_a, response_b \nwhile checking if the text can be encoded later\nAfter that we will split data into train , test \nNow we will select a model for fine tuning  \nThere are 3 types of models \n* Encoder only (Deberta, Bert, RoBerta)\n* Encoder-Decoder (Flan t-5, T-5 , UL2/Flan-UL2)\n* Decoder (GPT2,3,Gemma 2, Gemma 3, DeepSeek-V2, DeepSeek3)<br>\nWe need decoder only Model->  (Deberta, Bert, RoBerta,Llama 3 Â· 8b-chat-hf)  We go with llama 3 cause good notebook score<br>\nBased on the model we have we will select the preprocesisng function\nUse this preprocessing function as part of the function to load data into the model ie the dataloader function\nWe also need a learning rate controller/ decay function\nNow to add learning rate and decayfunction <br> \nOptions\n* cos -> reduction of lr using cosine function.\n* step -> reduction using cos function.\n* exp -> reduction Exponentially.\n* LDZ -> linear decay to zero(current best)\nafter that we will add params like checkpoints , callbacks and loss function which will be log loss which is used to evaluate the performance of the model in this competition ","metadata":{}},{"cell_type":"markdown","source":"\nWhere:\n- **0** â†’ response_a wins  \n- **1** â†’ response_b wins  \n- **2** â†’ tie  \n\n---\n\n## âœ… Step 1: Target Column Preparation\n\n- The dataset contains **one-hot encoded labels** for the winner.\n- Convert these into a **single categorical column** named `winner_class` with values:\n  - `response_a`\n  - `response_b`\n  - `tie`\n\n---\n\n## âœ… Step 2: Input Text Construction\n\n- Combine the following fields into one input text:\n  - `prompt`\n  - `response_a`\n  - `response_b`\n- Ensure the combined text:\n  - Is clean and well-formatted\n  - Can be tokenized properly by the model tokenizer\n\n---\n\n## âœ… Step 3: Trainâ€“Test Split\n\n- Split the dataset into:\n  - **Training set**\n  - **Validation/Test set**\n\nThis helps ensure proper evaluation and prevents data leakage.\n\n---\n\n## âœ… Step 4: Model Selection\n\nWe consider three major transformer architectures:\n\n### ðŸ”¹ Encoder-only Models\n- DeBERTa  \n- BERT  \n- RoBERTa  \n\n### ðŸ”¹ Encoderâ€“Decoder Models\n- T5  \n- FLAN-T5  \n- UL2 / FLAN-UL2  \n\n### ðŸ”¹ Decoder-only Models\n- GPT-2 / GPT-3  \n- Gemma 2 / Gemma 3  \n- DeepSeek-V2 / DeepSeek-V3  \n- **LLaMA 3 (8B-Chat)**  \n\nâœ… **Selected Model:**  \n**LLaMA-3-8B-Chat**, chosen for strong performance and good Kaggle notebook compatibility.\n\n---\n\n## âœ… Step 5: Preprocessing Function\n\n- Choose a preprocessing function compatible with the selected model.\n- Tokenization should handle:\n  - Prompt + both responses\n  - Padding and truncation\n- This preprocessing function will be integrated into the **data loader**.\n\n---\n\n## âœ… Step 6: Learning Rate Scheduler\n\nA learning rate decay strategy is applied to improve training stability.\n\nAvailable options:\n\n- **Cosine decay (`cos`)** â€“ smooth cosine-based reduction  \n- **Step decay (`step`)** â€“ step-wise reduction  \n- **Exponential decay (`exp`)** â€“ exponential reduction  \n- **Linear Decay to Zero (`LDZ`)** â€“ *recommended (best performing)*  \n\n---\n\n## âœ… Step 7: Training Configuration\n\nConfigure the following components:\n\n- Optimizer  \n- Learning rate scheduler  \n- Callbacks  \n- Model checkpoints  \n- Loss function  \n\n### ðŸ“‰ Loss Function\n- **Log Loss (Cross-Entropy Loss)**  \n  Used as the evaluation metric for this competition.\n\n---\n\n## âœ… Final Pipeline Summary\n\n1. Prepare and clean the dataset  \n2. Generate winner class labels  \n3. Combine prompt and responses  \n4. Tokenize using model-specific preprocessing  \n5. Split data into train and validation sets  \n6. Load the LLaMA-3 model  \n7. Apply learning rate decay  \n8. Train with checkpoints and callbacks  \n9. Evaluate using log loss  \n","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed = 40 \n    model = \"deberta_v3_extra_small_en\"\n    sequence_len = 512\n    epochs  = 3 # needs only 1-3 epochs to avoid overfitting\n    batch_size = 16 #\n    scheduler  = 'cosine'\n    labelname = { 0 : 'winner_model_a' ,1 : 'winner_model_b',2 : 'winner_tie'  }\n    namelabel = {v:k for k,v in labelname.items()}\n    classlabel= list(labelname.keys())\n    classnames = list(labelname.values())\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:37:49.199303Z","iopub.execute_input":"2025-12-21T13:37:49.199958Z","iopub.status.idle":"2025-12-21T13:37:49.204404Z","shell.execute_reply.started":"2025-12-21T13:37:49.199934Z","shell.execute_reply":"2025-12-21T13:37:49.203806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:37:50.020674Z","iopub.execute_input":"2025-12-21T13:37:50.021243Z","iopub.status.idle":"2025-12-21T13:37:50.024690Z","shell.execute_reply.started":"2025-12-21T13:37:50.021222Z","shell.execute_reply":"2025-12-21T13:37:50.023958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keras.mixed_precision.set_global_policy(\"mixed_float16\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:37:50.422812Z","iopub.execute_input":"2025-12-21T13:37:50.423302Z","iopub.status.idle":"2025-12-21T13:37:50.426531Z","shell.execute_reply.started":"2025-12-21T13:37:50.423279Z","shell.execute_reply":"2025-12-21T13:37:50.425838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/llm-classification-finetuning'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:37:51.089040Z","iopub.execute_input":"2025-12-21T13:37:51.089637Z","iopub.status.idle":"2025-12-21T13:37:51.093445Z","shell.execute_reply.started":"2025-12-21T13:37:51.089613Z","shell.execute_reply":"2025-12-21T13:37:51.092724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(f'{BASE_PATH}/train.csv') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:37:51.800679Z","iopub.execute_input":"2025-12-21T13:37:51.801202Z","iopub.status.idle":"2025-12-21T13:37:53.501503Z","shell.execute_reply.started":"2025-12-21T13:37:51.801182Z","shell.execute_reply":"2025-12-21T13:37:53.500887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:08:59.887443Z","iopub.execute_input":"2025-12-21T13:08:59.887656Z","iopub.status.idle":"2025-12-21T13:08:59.896551Z","shell.execute_reply.started":"2025-12-21T13:08:59.887639Z","shell.execute_reply":"2025-12-21T13:08:59.895807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.prompt[4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:09:01.173719Z","iopub.execute_input":"2025-12-21T13:09:01.174026Z","iopub.status.idle":"2025-12-21T13:09:01.180366Z","shell.execute_reply.started":"2025-12-21T13:09:01.174004Z","shell.execute_reply":"2025-12-21T13:09:01.179775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Train Data\ndf = pd.read_csv(f'{BASE_PATH}/train.csv') \n\n\n# Prompts are shown as string representation of a list or tuple in which the prompt is at it's 0th position\ndf[\"prompt\"] = df.prompt.map(lambda x: eval(x)[0])\ndf[\"response_a\"] = df.response_a.map(lambda x: eval(x.replace(\"null\",\"''\"))[0])\ndf[\"response_b\"] = df.response_b.map(lambda x: eval(x.replace(\"null\", \"''\"))[0])\n\n# Label conversion\ndf[\"class_name\"] = df[[\"winner_model_a\", \"winner_model_b\" , \"winner_tie\"]].idxmax(axis=1)\ndf[\"class_label\"] = df.class_name.map(CFG.namelabel)\ndf['label_arr'] = df[[\"winner_model_a\", \"winner_model_b\" , \"winner_tie\"]].values.tolist()\n# Show Sample\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:39:31.517257Z","iopub.execute_input":"2025-12-21T13:39:31.517527Z","iopub.status.idle":"2025-12-21T13:39:37.129896Z","shell.execute_reply.started":"2025-12-21T13:39:31.517506Z","shell.execute_reply":"2025-12-21T13:39:37.129152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Load Test Data\ntest_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n\n# Take the first prompt and response\ntest_df[\"prompt\"] = test_df.prompt.map(lambda x: eval(x)[0])\ntest_df[\"response_a\"] = test_df.response_a.map(lambda x: eval(x.replace(\"null\",\"''\"))[0])\ntest_df[\"response_b\"] = test_df.response_b.map(lambda x: eval(x.replace(\"null\", \"''\"))[0])\n\n# Show Sample\ntest_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:39:41.300621Z","iopub.execute_input":"2025-12-21T13:39:41.300889Z","iopub.status.idle":"2025-12-21T13:39:41.314404Z","shell.execute_reply.started":"2025-12-21T13:39:41.300870Z","shell.execute_reply":"2025-12-21T13:39:41.313866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Define a function to create options based on the prompt and choices\ndef make_pairs(row):\n    row[\"encode_fail\"] = False\n    try:\n        prompt = row.prompt.encode(\"utf-8\").decode(\"utf-8\")\n    except:\n        prompt = \"\"\n        row[\"encode_fail\"] = True\n\n    try:\n        response_a = row.response_a.encode(\"utf-8\").decode(\"utf-8\")\n    except:\n        response_a = \"\"\n        row[\"encode_fail\"] = True\n\n    try:\n        response_b = row.response_b.encode(\"utf-8\").decode(\"utf-8\")\n    except:\n        response_b = \"\"\n        row[\"encode_fail\"] = True\n        \n    row['options'] = [f\"Prompt: {prompt}\",f\"\\n\\nResponse_a: {response_a}\",  # Response from Model A\n                      f\"Response_b: {response_b}\"  # Response from Model B\n                     ]\n    return row\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T14:15:09.406201Z","iopub.execute_input":"2025-12-21T14:15:09.406825Z","iopub.status.idle":"2025-12-21T14:15:09.411767Z","shell.execute_reply.started":"2025-12-21T14:15:09.406790Z","shell.execute_reply":"2025-12-21T14:15:09.411132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df\ndisplay(df.head(2))  # Display the first 2 rows of df\n\ntest_df = test_df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df\ndisplay(test_df.head(2))  # Display the first 2 rows of df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T14:15:16.875053Z","iopub.execute_input":"2025-12-21T14:15:16.875620Z","iopub.status.idle":"2025-12-21T14:15:22.200185Z","shell.execute_reply.started":"2025-12-21T14:15:16.875600Z","shell.execute_reply":"2025-12-21T14:15:22.199520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.encode_fail.value_counts(normalize=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T14:15:23.244409Z","iopub.execute_input":"2025-12-21T14:15:23.244895Z","iopub.status.idle":"2025-12-21T14:15:23.251939Z","shell.execute_reply.started":"2025-12-21T14:15:23.244870Z","shell.execute_reply":"2025-12-21T14:15:23.251216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"model_df = pd.concat([df.model_a, df.model_b])\ncounts = model_df.value_counts().reset_index()\ncounts.columns = ['LLM','Count']\n\nfig = px.bar(counts ,x='LLM', y ='Count',\n             title = 'Distribution of LLMs',\n             color = 'Count' , color_continuous_scale = 'viridis'\n            )\nfig.update_layout(xaxis_tickangle = -45)\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:40:53.010520Z","iopub.execute_input":"2025-12-21T13:40:53.010791Z","iopub.status.idle":"2025-12-21T13:40:53.062853Z","shell.execute_reply.started":"2025-12-21T13:40:53.010771Z","shell.execute_reply":"2025-12-21T13:40:53.062311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:40:56.999515Z","iopub.execute_input":"2025-12-21T13:40:57.000179Z","iopub.status.idle":"2025-12-21T13:40:57.007612Z","shell.execute_reply.started":"2025-12-21T13:40:57.000156Z","shell.execute_reply":"2025-12-21T13:40:57.006850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"win_resps = pd.DataFrame()\nwin_resps['resps'] = pd.concat([df[df['class_name']=='winner_model_a']['response_a'],df[df['class_name']=='winner_model_b']['response_b']])\nwin_resps['lens'] = win_resps['resps'].apply(lambda x:len(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:41:38.079239Z","iopub.execute_input":"2025-12-21T13:41:38.079502Z","iopub.status.idle":"2025-12-21T13:41:38.134037Z","shell.execute_reply.started":"2025-12-21T13:41:38.079483Z","shell.execute_reply":"2025-12-21T13:41:38.133466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len_freq = win_resps['lens'].value_counts().sort_index()\nplt.hist(len_freq , color='skyblue', edgecolor='black')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:41:39.326466Z","iopub.execute_input":"2025-12-21T13:41:39.327140Z","iopub.status.idle":"2025-12-21T13:41:39.543216Z","shell.execute_reply.started":"2025-12-21T13:41:39.327119Z","shell.execute_reply":"2025-12-21T13:41:39.542671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:41:41.491170Z","iopub.execute_input":"2025-12-21T13:41:41.491427Z","iopub.status.idle":"2025-12-21T13:41:41.495095Z","shell.execute_reply.started":"2025-12-21T13:41:41.491409Z","shell.execute_reply":"2025-12-21T13:41:41.494312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size = 0.2 , stratify = df['class_label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:41:42.657496Z","iopub.execute_input":"2025-12-21T13:41:42.657975Z","iopub.status.idle":"2025-12-21T13:41:42.731830Z","shell.execute_reply.started":"2025-12-21T13:41:42.657952Z","shell.execute_reply":"2025-12-21T13:41:42.731260Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:09:57.166838Z","iopub.status.idle":"2025-12-21T13:09:57.167169Z","shell.execute_reply.started":"2025-12-21T13:09:57.167005Z","shell.execute_reply":"2025-12-21T13:09:57.167020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:09:57.168399Z","iopub.status.idle":"2025-12-21T13:09:57.168731Z","shell.execute_reply.started":"2025-12-21T13:09:57.168568Z","shell.execute_reply":"2025-12-21T13:09:57.168583Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"preprocessor  = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n    preset = CFG.model,\n    sequence_length=CFG.sequence_len,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:41:46.043664Z","iopub.execute_input":"2025-12-21T13:41:46.043943Z","iopub.status.idle":"2025-12-21T13:41:48.590074Z","shell.execute_reply.started":"2025-12-21T13:41:46.043924Z","shell.execute_reply":"2025-12-21T13:41:48.589434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outs = preprocessor(df.options.iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:41:48.591291Z","iopub.execute_input":"2025-12-21T13:41:48.591559Z","iopub.status.idle":"2025-12-21T13:41:48.676125Z","shell.execute_reply.started":"2025-12-21T13:41:48.591538Z","shell.execute_reply":"2025-12-21T13:41:48.675596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for k, v in outs.items():\n    print(k, \":\", v.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:41:48.676723Z","iopub.execute_input":"2025-12-21T13:41:48.676887Z","iopub.status.idle":"2025-12-21T13:41:48.681202Z","shell.execute_reply.started":"2025-12-21T13:41:48.676874Z","shell.execute_reply":"2025-12-21T13:41:48.680486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Build Dataset","metadata":{}},{"cell_type":"markdown","source":"https://www.tensorflow.org/guide/data","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def build_dataset(texts, batch_size =32 , cache = True , shuffle = True):\n    slices= texts, keras.uitils.to_categorical(label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_fn(text, label=None):\n    text = preprocessor(text)  # Preprocess text\n    return (text, label) if label is not None else text  # Return processed text and label if available","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:42:02.316516Z","iopub.execute_input":"2025-12-21T13:42:02.316759Z","iopub.status.idle":"2025-12-21T13:42:02.320565Z","shell.execute_reply.started":"2025-12-21T13:42:02.316742Z","shell.execute_reply":"2025-12-21T13:42:02.319807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_texts = train_df.options.tolist()  # Extract training texts\ntrain_labels = train_df.class_label.tolist()  # Extract training labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:42:02.645347Z","iopub.execute_input":"2025-12-21T13:42:02.645838Z","iopub.status.idle":"2025-12-21T13:42:02.652015Z","shell.execute_reply.started":"2025-12-21T13:42:02.645817Z","shell.execute_reply":"2025-12-21T13:42:02.651387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_texts = val_df.options.tolist()  # Extract validation texts\nvalid_labels = val_df.class_label.tolist()  # Extract validation labels\nvalid_ds = build_dataset(valid_texts, valid_labels,\n                         batch_size=CFG.batch_size,\n                         shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T13:42:03.136379Z","iopub.execute_input":"2025-12-21T13:42:03.137031Z","iopub.status.idle":"2025-12-21T13:42:03.157791Z","shell.execute_reply.started":"2025-12-21T13:42:03.136979Z","shell.execute_reply":"2025-12-21T13:42:03.156908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.iloc[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:16:20.254964Z","iopub.execute_input":"2025-11-30T14:16:20.255526Z","iopub.status.idle":"2025-11-30T14:16:20.261592Z","shell.execute_reply.started":"2025-11-30T14:16:20.255505Z","shell.execute_reply":"2025-11-30T14:16:20.261027Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now to add learning rate and decayfunction <br> \nOptions\n* cos -> reduction of lr using cosine function.\n* step -> reduction using cos function.\n* exp -> reduction Exponentially.\n* LDZ -> linear decay to zero(current best)","metadata":{}},{"cell_type":"code","source":"import math\n\ndef get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6\n    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8\n\n    def lrfn(epoch):  # Learning rate update function\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n        elif mode == 'cos':\n            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index / decay_total_epochs\n            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n        return lr\n\n    if plot:  # Plot lr curve if plot is True\n        plt.figure(figsize=(10, 5))\n        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('lr')\n        plt.title('LR Scheduler')\n        plt.show()\n\n    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr_cb = get_lr_callback(CFG.batch_size, plot=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',\n                                          monitor='val_log_loss',# \n                                          save_best_only=True,# best model save \n                                          save_weights_only=True,\n                                          mode='min')  # Get Model checkpoint callback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As Log loss is used in competitions we use log loss for model eval ","metadata":{}},{"cell_type":"code","source":"log_loss = keras.metrics.CategoricalCrossentropy(name=\"log_loss\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"llama 3 model","metadata":{}},{"cell_type":"code","source":"import keras\nimport keras_hub\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"llama_lm = keras_hub.models.Llama3CausalLM.from_preset(\"llama3_8b_en_int8\", dtype=\"bfloat16\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keras.config.set_dtype_policy(\"bfloat16\")\nllama_lm = keras_hub.models.Llama3CausalLM.from_preset(\"llama3_instruct_8b_en\")\nllama_lm.generate(\"What is Keras?\", max_length=500)\n\n# Generate with batched prompts.\nllama_lm.generate([\"What is Keras?\", \"Give me your best brownie recipe.\"], max_length=500)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}